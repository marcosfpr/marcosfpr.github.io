<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Deep Leakage from Gradients</title>
    <link rel="stylesheet" href="../style.css">
    <script defer src="../theme.js"></script>

    <script>
        window.MathJax = {
            tex: {
                inlineMath: [['\\(', '\\)']],
                displayMath: [['\\[', '\\]']],
                processEscapes: true
            }
        };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
</head>

<body>
    <header>
        <h1>marcosfpr</h1>
        <nav>
            <a href="../index.html">Home</a> |
            <a href="index.html">Wiki</a> |
            <button id="theme-toggle" aria-label="Toggle light/dark">Light/Dark</button>
        </nav>
    </header>

    <main>
        
<div id="Deep Leakage from Gradients"><h1 id="Deep Leakage from Gradients" class="header"><a href="#Deep Leakage from Gradients">Deep Leakage from Gradients</a></h1></div>

<p>
A review of the paper <a href="https://arxiv.org/abs/1906.08935">Deep Leakage from Gradients</a> by <span id="Deep Leakage from Gradients-Zhu et al. (2019)"></span><strong id="Zhu et al. (2019)">Zhu et al. (2019)</strong>.
</p>

<p>
Exchanging gradients is widely used in modern collaborative machine learning
(e.g., distributed training, federated learning). For a long time, people believed
gradients were safe to share. However, <span id="Deep Leakage from Gradients-Zhu et al. (2019)"></span><strong id="Zhu et al. (2019)">Zhu et al. (2019)</strong> showed that it’s possible
to recover private training data from publicly shared gradients — an attack they named
<span id="Deep Leakage from Gradients-Deep Leakage from Gradients"></span><strong id="Deep Leakage from Gradients">Deep Leakage from Gradients</strong> (DLG).
</p>

<div id="Deep Leakage from Gradients-Attack Model"><h2 id="Attack Model" class="header"><a href="#Deep Leakage from Gradients-Attack Model">Attack Model</a></h2></div>
<p>
Distributed and collaborative training are common in large-scale ML tasks. The central
question: <span id="Deep Leakage from Gradients-Attack Model-does “gradient sharing” protect the privacy of each participant’s dataset?"></span><strong id="does “gradient sharing” protect the privacy of each participant’s dataset?">does “gradient sharing” protect the privacy of each participant’s dataset?</strong>
</p>

<p>
Prior work showed gradients can leak <span id="Deep Leakage from Gradients-Attack Model-properties"></span><strong id="properties">properties</strong> of data (e.g., whether a sample with some attribute
is in a batch). DLG considers a stronger scenario: *can we reconstruct the
training data itself from gradients?* Given a model \(F(\cdot)\), weights \(W\), and gradients \(\nabla W\) computed on
a (input, label) pair, the answer is <span id="Deep Leakage from Gradients-Attack Model-yes"></span><strong id="yes">yes</strong>.
</p>

<div id="Deep Leakage from Gradients-Attack Model-High-Level Idea"><h3 id="High-Level Idea" class="header"><a href="#Deep Leakage from Gradients-Attack Model-High-Level Idea">High-Level Idea</a></h3></div>
<p>
DLG presents an optimization procedure that recovers both inputs and labels in a small number of iterations.
</p>

<ol>
<li>
Randomly initialize a <span id="Deep Leakage from Gradients-Attack Model-High-Level Idea-dummy"></span><strong id="dummy">dummy</strong> input \(x'\) and label \(y'\).

<li>
Compute <span id="Deep Leakage from Gradients-Attack Model-High-Level Idea-dummy"></span><strong id="dummy">dummy</strong> gradients from \(x', y'\).

<li>
Instead of updating model weights, update \(x', y'\) so that dummy gradients match the <span id="Deep Leakage from Gradients-Attack Model-High-Level Idea-real"></span><strong id="real">real</strong> gradients.

<li>
Iterate until convergence; the optimized \(x', y'\) approximate the true training example.

</ol>
<p>
This is a <span id="Deep Leakage from Gradients-Attack Model-High-Level Idea-white-box inference attack"></span><strong id="white-box inference attack">white-box inference attack</strong>: the adversary knows the model architecture and parameters
and has the gradients w.r.t. a private (input, label) pair.
</p>

<div id="Deep Leakage from Gradients-Attack Model-Attack Formulation"><h3 id="Attack Formulation" class="header"><a href="#Deep Leakage from Gradients-Attack Model-Attack Formulation">Attack Formulation</a></h3></div>
<p>
At step \(t\), each node \(i\) samples a minibatch \((x_{t,i}, y_{t,i})\) and computes:
</p>

\begin{align}
\nabla W_{t,i} = \frac{\partial \,\ell(F(x_{t,i}; W),\, y_{t,i})}{\partial W}
\end{align}

<p>
The gradients are averaged and used to update weights:
</p>

\begin{align}
\overline{\nabla W_t} = \frac{1}{N}\sum_{j=1}^{N}\nabla W_{t,j}
\quad;\quad
W_{t+1} = W_t - \eta\,\overline{\nabla W_t}
\end{align}

<p>
Given another participant’s gradients \(\nabla W_{t,k}\), the attacker aims to recover \((x_{t,k}, y_{t,k})\).
</p>

<p>
Initialize dummy data \(x', y'\) and compute dummy gradients:
</p>

\begin{align}
\nabla W' = \frac{\partial \,\ell(F(x'; W),\, y')}{\partial W}
\end{align}

<p>
Then optimize \(x', y'\) to minimize the gradient distance:
</p>

\begin{align}
(x'^*, y'^*) = \arg\min_{x',y'} \left\| \nabla W' - \nabla W \right\|^2
= \arg\min_{x',y'} \left\| \frac{\partial \,\ell(F(x'; W),\, y')}{\partial W} - \nabla W \right\|^2
\end{align}

<p>
Notes:
</p>

<ul>
<li>
The distance is differentiable w.r.t. \(x', y'\), enabling gradient-based optimization.

<li>
This requires <span id="Deep Leakage from Gradients-Attack Model-Attack Formulation-second-order derivatives"></span><strong id="second-order derivatives">second-order derivatives</strong>; DLG assumes \(F(\cdot)\) is twice
  differentiable (true for most deep nets).

<li>
The paper uses <span id="Deep Leakage from Gradients-Attack Model-Attack Formulation-L-BFGS"></span><strong id="L-BFGS">L-BFGS</strong> (learning rate 1, history size 100). For images they optimize
  ~1200 iterations; for text ~100 iterations (typical values from the paper).

</ul>
<p>
<span id="Deep Leakage from Gradients-Attack Model-Attack Formulation-Algorithm (DLG)"></span><strong id="Algorithm (DLG)">Algorithm (DLG)</strong>
</p>

<ol>
<li>
Randomly initialize \(x'\), \(y'\).

<li>
Compute \(\nabla W'\).

<li>
Update \(x', y'\) to reduce \(\|\nabla W' - \nabla W\|^2\).

<li>
Repeat 2–3 until convergence.

<li>
Return optimized \(x', y'\).

</ol>
<div id="Deep Leakage from Gradients-Experiments"><h2 id="Experiments" class="header"><a href="#Deep Leakage from Gradients-Experiments">Experiments</a></h2></div>
<p>
DLG reconstructs training data with high fidelity on both <span id="Deep Leakage from Gradients-Experiments-image"></span><strong id="image">image</strong> and <span id="Deep Leakage from Gradients-Experiments-text"></span><strong id="text">text</strong> tasks.
</p>

<ul>
<li>
<span id="Deep Leakage from Gradients-Experiments-Vision:"></span><strong id="Vision:">Vision:</strong> MNIST, CIFAR-100, SVHN, LFW with a ResNet-56 backbone.

<li>
<span id="Deep Leakage from Gradients-Experiments-Text:"></span><strong id="Text:">Text:</strong> Masked Language Modeling with <span id="Deep Leakage from Gradients-Experiments-BERT"></span><strong id="BERT">BERT</strong>. Because tokens are discrete, DLG optimizes <span id="Deep Leakage from Gradients-Experiments-embeddings"></span><strong id="embeddings">embeddings</strong> (continuous) and then <span id="Deep Leakage from Gradients-Experiments-inverts"></span><strong id="inverts">inverts</strong> them by choosing the nearest token in the embedding matrix after optimization.

</ul>
<div id="Deep Leakage from Gradients-Final Remarks"><h2 id="Final Remarks" class="header"><a href="#Deep Leakage from Gradients-Final Remarks">Final Remarks</a></h2></div>

<div id="Deep Leakage from Gradients-Final Remarks-Batched Gradients"><h3 id="Batched Gradients" class="header"><a href="#Deep Leakage from Gradients-Final Remarks-Batched Gradients">Batched Gradients</a></h3></div>
<p>
The attack is strongest when the batch size is 1. With batch size \(N&gt;1\), naive optimization converges slowly (intuitively due to the \(N!\) permutations).
</p>

<p>
DLG’s tweak: update <span id="Deep Leakage from Gradients-Final Remarks-Batched Gradients-one"></span><strong id="one">one</strong> sample’s dummy variables at a time.
</p>

<p>
Let \(\mathbb{D}_t = \|\nabla W'_t - \nabla W\|^2\). For batch size 1:
</p>

\begin{align}
x'_{t+1} \leftarrow x'_t - \eta\, \nabla_{x'_t} \mathbb{D}_t
\quad;\quad
y'_{t+1} \leftarrow y'_t - \eta\, \nabla_{y'_t} \mathbb{D}_t
\end{align}

<p>
For batch size \(N&gt;1\), cycle through samples:
</p>

\begin{align}
x'^{\,i \bmod N}_{t+1} &amp;\leftarrow x'^{\,i \bmod N}_t - \eta\, \nabla_{x'^{\,i \bmod N}_t}\, \mathbb{D}_t\\
y'^{\,i \bmod N}_{t+1} &amp;\leftarrow y'^{\,i \bmod N}_t - \eta\, \nabla_{y'^{\,i \bmod N}_t}\, \mathbb{D}_t
\end{align}

<p>
This yields faster, more stable convergence.
</p>

<div id="Deep Leakage from Gradients-Final Remarks-Possible Defenses"><h3 id="Possible Defenses" class="header"><a href="#Deep Leakage from Gradients-Final Remarks-Possible Defenses">Possible Defenses</a></h3></div>

<ul>
<li>
<span id="Deep Leakage from Gradients-Final Remarks-Possible Defenses-Gradient noise:"></span><strong id="Gradient noise:">Gradient noise:</strong> Add noise before sharing (privacy ↑, <span id="Deep Leakage from Gradients-Final Remarks-Possible Defenses-can"></span><strong id="can">can</strong> hurt accuracy).

<li>
<span id="Deep Leakage from Gradients-Final Remarks-Possible Defenses-Gradient compression/quantization:"></span><strong id="Gradient compression/quantization:">Gradient compression/quantization:</strong> Compress before sharing (effective in experiments).

<li>
<span id="Deep Leakage from Gradients-Final Remarks-Possible Defenses-Large batch sizes:"></span><strong id="Large batch sizes:">Large batch sizes:</strong> Reduce attack effectiveness.

<li>
<span id="Deep Leakage from Gradients-Final Remarks-Possible Defenses-Upscale images:"></span><strong id="Upscale images:">Upscale images:</strong> Makes reconstruction harder.

<li>
<span id="Deep Leakage from Gradients-Final Remarks-Possible Defenses-Encrypt gradients:"></span><strong id="Encrypt gradients:">Encrypt gradients:</strong> Share only encrypted gradients.

</ul>

    </main>

    <footer>
        &copy; 2024 marcosfpr
    </footer>
</body>

</html>
